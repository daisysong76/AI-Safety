# Training Methods Overview

## Feature/Aspect Comparison

| Feature/Aspect                       | DPOTrainer                                           | SFTTrainer (sft_trainer.py)                          | AdvancedBiasAwareLLMTrainer                          | PRM Trainer                                           |
|--------------------------------------|-----------------------------------------------------|------------------------------------------------------|------------------------------------------------------|------------------------------------------------------|
| **Training Objective**               | Direct Preference Optimization based on preferences/rewards | Supervised Fine-Tuning (SFT) with language modeling   | Supervised Fine-Tuning (SFT) with advanced bias mitigation | Fine-tune primary model based on evaluations from a Post-Training Reward Model (PRM) |
| **Loss Function**                    | DPOLoss (preference loss), optional auxiliary and NLL losses | GPTLMLoss (language modeling loss), optional auxiliary losses | GPTLMLoss, optional auxiliary losses                  | Reinforcement Learning loss (e.g., PPO), guided by PRM's reward scores |
| **Reference Model**                  | Yes, utilizes a reference model                      | No                                                   | No                                                   | Yes, may utilize a reference model or the PRM itself as a guide |
| **Bias Mitigation**                  | No                                                  | No, but can be integrated                            | Yes, comprehensive bias mitigation                    | Indirectly, through PRM's reward criteria which can include fairness and bias-related metrics |
| **Parameter-Efficient Fine-Tuning (PEFT)** | No                                                  | Not explicitly, but can be integrated                | Yes, via LoRA                                       | Depends on Implementation, can integrate PEFT methods like LoRA |
| **Framework Integration**            | Custom training loop                                 | Custom training loop with manual steps               | HuggingFace's Trainer class                           | Combination of custom training loops for reinforcement learning and integration with HuggingFace's frameworks |
| **Logging and Monitoring**           | WandB and TensorBoard                               | WandB and TensorBoard                                | WandB                                               | WandB and possibly TensorBoard, depending on implementation |
| **Checkpointing**                    | DeepSpeed and HuggingFace formats                   | DeepSpeed and HuggingFace formats                    | HuggingFace format                                   | HuggingFace and/or DeepSpeed formats, with potential additional checkpoints for PRM |
| **Distributed Training Support**     | Yes, with strategies like ring attention groups     | Yes, with Strategy class and DistributedSampler      | Assumes distributed training via HuggingFace's Trainer | Yes, often requires distributed training support for handling large models and multiple GPUs/nodes |
| **Gradient Clipping**                | Yes, controlled via max_norm                        | Yes, controlled via max_norm                         | Yes, controlled via max_norm                          | Yes, typically controlled via max_norm or other gradient stabilization techniques |
| **Auxiliary Losses**                 | Optional, based on aux_loss_coef                   | Optional, based on aux_loss_coef                     | Optional, based on aux_loss_coef                     | Optional, can include auxiliary objectives alongside reinforcement learning losses |
| **Sample Packing**                   | Yes, supports packing of samples for efficiency     | Yes, controlled via packing_samples                   | No                                                   | No, unless integrated with custom data handling strategies |
| **Bias-Aware Components**            | No                                                  | No                                                   | Alignment Constructor, Response Generator, Mixture Optimizer, Gender Debiasing | Indirectly, through PRM's reward criteria which can be designed to mitigate biases |
| **Ease of Use**                      | Requires manual setup and deeper understanding      | Requires manual setup and deeper understanding       | More user-friendly with HuggingFace's Trainer abstraction | Moderate, requires understanding of both reinforcement learning and existing training frameworks |
| **PEFT Integration**                 | No                                                  | Not implemented, but compatible                      | Implemented via LoRA                                 | Possible, can incorporate PEFT methods for efficiency |
| **Data Handling**                    | Handles both chosen (preferred) and rejected (less preferred) samples | Handles labeled data with optional sample packing     | Prepares debiased datasets, handles tokenization, and manages data splits | Utilizes PRM to evaluate and score outputs, may require additional data handling for reward assignments |
| **Training Loop Customization**      | Highly customizable with manual control over training steps and components | Customizable with manual training steps              | Leverages HuggingFace's Trainer for streamlined training workflows | Combines custom reinforcement learning loops with existing training frameworks, allowing for flexible customization |
| **Evaluation Metrics**               | Preference accuracy, loss metrics                   | Language modeling loss, auxiliary loss metrics        | Bias evaluation metrics, language modeling loss       | Reward scores from PRM, language modeling metrics, potentially bias-related metrics |
